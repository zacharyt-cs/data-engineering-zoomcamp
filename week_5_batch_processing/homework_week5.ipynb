{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Install Spark and PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. HVFHW February 2021\n",
    "Download the HVFHV data for february 2021:\n",
    "\n",
    "`wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv`\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons. We will use this dataset for all the remaining questions.\n",
    "\n",
    "Repartition it to 24 partitions and save it to parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "# define schema\n",
    "taxi_schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data with spark\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(taxi_schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 08:03:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/02 08:03:30 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/02 08:03:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# repartition data and parquetize\n",
    "df.repartition(24) \\\n",
    "    .write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Count Records\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('pickup_date', F.to_date(df.pickup_datetime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feb15 = df.filter(df.pickup_date == '2021-02-15')\n",
    "df_feb15.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Longest trip for each day\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhv_feb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'pickup_date']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+------------------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|     trip_duration|\n",
      "+-----------------+-------------------+-------------------+------------------+\n",
      "|           HV0005|2021-02-11 13:40:44|2021-02-12 10:39:44|            1259.0|\n",
      "|           HV0004|2021-02-17 15:54:53|2021-02-18 07:48:34| 953.6833333333333|\n",
      "|           HV0004|2021-02-20 12:08:15|2021-02-21 00:22:14| 733.9833333333333|\n",
      "|           HV0003|2021-02-03 20:24:25|2021-02-04 07:41:58|            677.55|\n",
      "|           HV0003|2021-02-19 23:17:44|2021-02-20 09:44:01| 626.2833333333333|\n",
      "|           HV0003|2021-02-25 17:13:35|2021-02-26 02:57:05|             583.5|\n",
      "|           HV0003|2021-02-20 01:36:13|2021-02-20 11:16:19|             580.1|\n",
      "|           HV0005|2021-02-18 15:24:19|2021-02-19 01:01:11| 576.8666666666667|\n",
      "|           HV0003|2021-02-18 01:31:20|2021-02-18 11:07:15| 575.9166666666666|\n",
      "|           HV0005|2021-02-10 20:51:39|2021-02-11 06:21:08| 569.4833333333333|\n",
      "|           HV0003|2021-02-10 01:56:17|2021-02-10 10:57:33| 541.2666666666667|\n",
      "|           HV0005|2021-02-25 09:18:18|2021-02-25 18:18:57|            540.65|\n",
      "|           HV0005|2021-02-21 19:59:13|2021-02-22 04:56:16|            537.05|\n",
      "|           HV0003|2021-02-09 18:36:13|2021-02-10 03:31:00| 534.7833333333333|\n",
      "|           HV0004|2021-02-06 09:48:09|2021-02-06 18:32:16| 524.1166666666667|\n",
      "|           HV0005|2021-02-02 09:42:30|2021-02-02 18:17:43| 515.2166666666667|\n",
      "|           HV0005|2021-02-10 10:12:08|2021-02-10 18:46:24| 514.2666666666667|\n",
      "|           HV0003|2021-02-09 13:30:13|2021-02-09 22:02:25|             512.2|\n",
      "|           HV0005|2021-02-21 22:50:52|2021-02-22 07:21:52|             511.0|\n",
      "|           HV0005|2021-02-05 21:32:33|2021-02-06 06:01:04|508.51666666666665|\n",
      "+-----------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT\n",
    "    hvfhs_license_num,\n",
    "    pickup_datetime,\n",
    "    dropoff_datetime,\n",
    "    (CAST(dropoff_datetime AS long) - CAST(pickup_datetime AS long))/60 AS trip_duration\n",
    "FROM\n",
    "    fhv_feb\n",
    "ORDER BY trip_duration DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5. Most frequent dispatching_base_num\n",
    "\n",
    "Now find the most frequently occurring dispatching_base_num in this dataset.\n",
    "\n",
    "How many stages this spark job has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  total|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "|              B02875| 685390|\n",
      "|              B02765| 559768|\n",
      "|              B02869| 429720|\n",
      "|              B02887| 322331|\n",
      "|              B02871| 312364|\n",
      "|              B02864| 311603|\n",
      "|              B02866| 311089|\n",
      "|              B02878| 305185|\n",
      "|              B02682| 303255|\n",
      "|              B02617| 274510|\n",
      "|              B02883| 251617|\n",
      "|              B02884| 244963|\n",
      "|              B02882| 232173|\n",
      "|              B02876| 215693|\n",
      "|              B02879| 210137|\n",
      "|              B02867| 200530|\n",
      "|              B02877| 198938|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    count(*) AS total\n",
    "FROM\n",
    "    fhv_feb\n",
    "GROUP BY dispatching_base_num\n",
    "ORDER BY total DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-02 10:33:20--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.165.224\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.165.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  41.4KB/s    in 0.3s    \n",
      "\n",
      "2022-03-02 10:33:22 (41.4 KB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "df.printSchema()\n",
    "df_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('trip_table')\n",
    "df_zone.registerTempTable('zone_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|         pickup_zone|        dropoff_zone|frequency|\n",
      "+--------------------+--------------------+---------+\n",
      "|       East New York|       East New York|    45041|\n",
      "|        Borough Park|        Borough Park|    37329|\n",
      "|            Canarsie|            Canarsie|    28026|\n",
      "| Crown Heights North| Crown Heights North|    25976|\n",
      "|           Bay Ridge|           Bay Ridge|    17934|\n",
      "|             Astoria|             Astoria|    14688|\n",
      "|     Jackson Heights|     Jackson Heights|    14688|\n",
      "|Central Harlem North|Central Harlem North|    14481|\n",
      "|      Bushwick South|      Bushwick South|    14424|\n",
      "|Flatbush/Ditmas Park|Flatbush/Ditmas Park|    13976|\n",
      "|    South Ozone Park|    South Ozone Park|    13716|\n",
      "|         Brownsville|         Brownsville|    12829|\n",
      "|         JFK Airport|                  NA|    12542|\n",
      "|Prospect-Lefferts...| Crown Heights North|    11814|\n",
      "|        Forest Hills|        Forest Hills|    11548|\n",
      "|      Bushwick North|      Bushwick South|    11491|\n",
      "|      Bushwick South|      Bushwick North|    11487|\n",
      "| Crown Heights North|Prospect-Lefferts...|    11462|\n",
      "| Crown Heights North|  Stuyvesant Heights|    11342|\n",
      "|Prospect-Lefferts...|Prospect-Lefferts...|    11308|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT\n",
    "    z1.Zone AS pickup_zone,\n",
    "    z2.Zone AS dropoff_zone,\n",
    "    count(*) AS frequency\n",
    "FROM\n",
    "    trip_table\n",
    "LEFT JOIN\n",
    "    zone_table AS z1\n",
    "ON (trip_table.PULocationID = z1.LocationID)\n",
    "LEFT JOIN\n",
    "    zone_table AS z2\n",
    "ON (trip_table.DOLocationID = z2.LocationID)\n",
    "GROUP BY \n",
    "    z1.Zone, z2.Zone\n",
    "ORDER BY 3 DESC\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
